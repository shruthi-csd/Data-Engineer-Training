{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Scenario: Employee Work Data for a Tech Company**\n",
        "\n",
        "**Step 1: Prepare Data in PySpark**"
      ],
      "metadata": {
        "id": "-WLLjf05Byqm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "l5ZAEIj7BQtk",
        "outputId": "fa9ba359-efd7-4f4b-cc2c-1012511b27d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7cd20d189fd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b200960258ba:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>EmployeeWorkData</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"EmployeeWorkData\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "data = [\n",
        "Row(EmpID=101, Name=\"Ravi\", Department=\"Engineering\", Project=\"AI Engine\", Salary=95000, HoursPerWeek=42),\n",
        "Row(EmpID=102, Name=\"Sneha\", Department=\"Engineering\", Project=\"Data Platform\", Salary=87000, HoursPerWeek=45),\n",
        "Row(EmpID=103, Name=\"Kabir\", Department=\"Marketing\", Project=\"Product Launch\", Salary=65000, HoursPerWeek=40),\n",
        "Row(EmpID=104, Name=\"Anita\", Department=\"Sales\", Project=\"Client Outreach\", Salary=70000, HoursPerWeek=38),\n",
        "Row(EmpID=105, Name=\"Divya\", Department=\"Engineering\", Project=\"AI Engine\", Salary=99000, HoursPerWeek=48),\n",
        "Row(EmpID=106, Name=\"Amit\", Department=\"Marketing\", Project=\"Social Media\", Salary=62000, HoursPerWeek=35),\n",
        "Row(EmpID=107, Name=\"Priya\", Department=\"HR\", Project=\"Policy Revamp\", Salary=58000, HoursPerWeek=37),\n",
        "Row(EmpID=108, Name=\"Manav\", Department=\"Sales\", Project=\"Lead Gen\", Salary=73000, HoursPerWeek=41),\n",
        "Row(EmpID=109, Name=\"Neha\", Department=\"Engineering\", Project=\"Security Suite\", Salary=91000, HoursPerWeek=46),\n",
        "Row(EmpID=110, Name=\"Farah\", Department=\"HR\", Project=\"Onboarding\", Salary=60000, HoursPerWeek=36)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOWSOocGCXlp",
        "outputId": "be765ebf-fac6-4c38-eded-7d95b581a64b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Create Views**\n",
        "\n",
        "Create a Local Temp View"
      ],
      "metadata": {
        "id": "fTg7zjQSCzLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"employees_local\")"
      ],
      "metadata": {
        "id": "NNpzWl6jC-f4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Global Temp View"
      ],
      "metadata": {
        "id": "BOpSq_PDDJLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceGlobalTempView(\"employees_global\")"
      ],
      "metadata": {
        "id": "GTj8apOcDIg9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part A: Exercises on Local View ( employees_local )**\n",
        "\n",
        "1. List all employees working on the \"AI Engine\" project."
      ],
      "metadata": {
        "id": "2Ybq4nEhDYf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM employees_local WHERE Project = 'AI Engine'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h6FwRe6DiUR",
        "outputId": "2a47036e-6d7d-4f50-8eab-21f6bbcaacbe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------+------+------------+\n",
            "|EmpID| Name| Department|  Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------+------+------------+\n",
            "|  101| Ravi|Engineering|AI Engine| 95000|          42|\n",
            "|  105|Divya|Engineering|AI Engine| 99000|          48|\n",
            "+-----+-----+-----------+---------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Show all employees from the \"Marketing\" department with salaries greater than 60,000."
      ],
      "metadata": {
        "id": "lh6Yc0IwDtx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM employees_local WHERE Department = 'Marketing' AND Salary >60000\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL70Ci4ED_pq",
        "outputId": "b67f635c-1950-4ccf-a3d1-ee57099f112c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+--------------+------+------------+\n",
            "|EmpID| Name|Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+----------+--------------+------+------------+\n",
            "|  103|Kabir| Marketing|Product Launch| 65000|          40|\n",
            "|  106| Amit| Marketing|  Social Media| 62000|          35|\n",
            "+-----+-----+----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Calculate the average salary for each department."
      ],
      "metadata": {
        "id": "7ShTDms9Efgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT Department, AVG(Salary) AS avg_sal FROM employees_local GROUP BY Department\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "felnwcKkEiFR",
        "outputId": "fecc0297-ae70-48d4-f3d3-4566bf19adbb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+\n",
            "| Department|avg_sal|\n",
            "+-----------+-------+\n",
            "|      Sales|71500.0|\n",
            "|Engineering|93000.0|\n",
            "|  Marketing|63500.0|\n",
            "|         HR|59000.0|\n",
            "+-----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. List the top 3 highest paid employees overall."
      ],
      "metadata": {
        "id": "xEBvA2SyFBvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM employees_local ORDER BY Salary DESC LIMIT 3\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrP6XQW9FFQR",
        "outputId": "c65df5c9-954e-42c6-eae1-4d5fb3e3ee86"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  109| Neha|Engineering|Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Find employees who work more than 40 hours per week."
      ],
      "metadata": {
        "id": "hDLAC17jFu8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM employees_local WHERE HoursPerWeek > 40\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fs7WAQcF3VZ",
        "outputId": "c0fcd151-c67c-463e-a22d-64edcf1806ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering| Data Platform| 87000|          45|\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  108|Manav|      Sales|      Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering|Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Group by project and display the number of employees per project."
      ],
      "metadata": {
        "id": "ArG6qppyGImp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT Project, COUNT(EmpID) AS No_of_emp FROM employees_local GROUP BY Project\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYDM50BFGMut",
        "outputId": "36497cab-61ea-44bc-e7ba-dedc0c28a16b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------+\n",
            "|        Project|No_of_emp|\n",
            "+---------------+---------+\n",
            "|  Data Platform|        1|\n",
            "|      AI Engine|        2|\n",
            "| Product Launch|        1|\n",
            "|Client Outreach|        1|\n",
            "| Security Suite|        1|\n",
            "|  Policy Revamp|        1|\n",
            "|       Lead Gen|        1|\n",
            "|   Social Media|        1|\n",
            "|     Onboarding|        1|\n",
            "+---------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Drop the local view. Try querying again — what happens?"
      ],
      "metadata": {
        "id": "ep2_gVU2Hypx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"DROP VIEW employees_local\")\n",
        "\n",
        "spark.sql(\"SELECT * FROM employees_local\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "cIMAkPGsH1pm",
        "outputId": "69768c2a-2e22-4ca0-f076-31a0d9d5be03"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `spark_catalog`.`default`.`employees_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1313924156.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DROP VIEW employees_local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM employees_local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                 )\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `spark_catalog`.`default`.`employees_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B: Exercises on Global View ( employees_global )**\n",
        "\n",
        "1. Retrieve all \"HR\" employees working fewer than 38 hours/week."
      ],
      "metadata": {
        "id": "mwtLHGTAIHCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_spark = SparkSession.builder.appName(\"EmployeeWorkData1\").getOrCreate()\n",
        "\n",
        "df.createOrReplaceGlobalTempView(\"employees_global\")\n",
        "\n",
        "new_spark.sql(\"SELECT * FROM global_temp.employees_global WHERE Department = 'HR' AND HoursPerWeek < 38\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTI_On0TILp3",
        "outputId": "a8fd27b7-8b33-4eac-ba99-4edf43a5911e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+-------------+------+------------+\n",
            "|EmpID| Name|Department|      Project|Salary|HoursPerWeek|\n",
            "+-----+-----+----------+-------------+------+------------+\n",
            "|  107|Priya|        HR|Policy Revamp| 58000|          37|\n",
            "|  110|Farah|        HR|   Onboarding| 60000|          36|\n",
            "+-----+-----+----------+-------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Calculate the total salary payout for each department."
      ],
      "metadata": {
        "id": "uKIzWgGJJabM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_spark.sql(\"SELECT Department, SUM(Salary) AS Tot_sal FROM global_temp.employees_global GROUP BY Department\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr2RiSg9JdvK",
        "outputId": "50c84334-a06d-403e-89fe-cb46dfa2460e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+\n",
            "| Department|Tot_sal|\n",
            "+-----------+-------+\n",
            "|      Sales| 143000|\n",
            "|Engineering| 372000|\n",
            "|  Marketing| 127000|\n",
            "|         HR| 118000|\n",
            "+-----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. For each employee, add a derived column Status :\n",
        "\n",
        "If HoursPerWeek > 45 → 'Overworked'\n",
        "Otherwise → 'Normal'"
      ],
      "metadata": {
        "id": "zWhl7w_7JrxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_spark.sql(\"SELECT *, CASE WHEN HoursPerWeek > 45 THEN 'Overworked' ELSE 'Normal' END AS Status FROM global_temp.employees_global\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HnVyuRMKJni",
        "outputId": "61ed8f5b-e08d-4435-ac04-e0b17dff5fe3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|    Status|\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|    Normal|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|    Normal|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|    Normal|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|    Normal|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|Overworked|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|    Normal|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|    Normal|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|    Normal|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|Overworked|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|    Normal|\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Count the total number of employees working on each \"Project\" ."
      ],
      "metadata": {
        "id": "tjY0jFnQKRdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_spark.sql(\"SELECT COUNT(EmpID) AS no_of_emp, Project FROM global_temp.employees_global GROUP BY Project\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF_RSLmDKUB3",
        "outputId": "c906cc26-356a-4778-a3cd-c0b19f8be9c2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------------+\n",
            "|no_of_emp|        Project|\n",
            "+---------+---------------+\n",
            "|        1|  Data Platform|\n",
            "|        2|      AI Engine|\n",
            "|        1| Product Launch|\n",
            "|        1|Client Outreach|\n",
            "|        1| Security Suite|\n",
            "|        1|  Policy Revamp|\n",
            "|        1|       Lead Gen|\n",
            "|        1|   Social Media|\n",
            "|        1|     Onboarding|\n",
            "+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. List employees whose salary is above the average salary in their department."
      ],
      "metadata": {
        "id": "2OMDE9YsLAqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_spark.sql(\"SELECT * FROM global_temp.employees_global WHERE Salary > (SELECT AVG(Salary) FROM global_temp.employees_global)\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZKYUd8BLEF5",
        "outputId": "a284dffe-18e6-40b6-b5a7-e468f13b1436"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering| Data Platform| 87000|          45|\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  109| Neha|Engineering|Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Open a new Spark session and query \"global_temp.employees_global\" from there."
      ],
      "metadata": {
        "id": "6zEnx4dILPzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_spark1 = SparkSession.builder.appName(\"EmployeeWorkData2\").getOrCreate()\n",
        "\n",
        "new_spark1.sql(\"SELECT * FROM global_temp.employees_global\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEkF2islLUTN",
        "outputId": "89725ca7-460a-48d7-b822-1574d2a93b1a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus Challenges**\n",
        "\n",
        "1. Use a window function to assign rank to employees within each department based on salary."
      ],
      "metadata": {
        "id": "WquYwQOxLrTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_spark.sql(\"SELECT *, RANK() OVER (PARTITION BY Department ORDER BY Salary DESC) AS rank FROM global_temp.employees_global\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdSV39oQLvBw",
        "outputId": "c0e6ac3e-9b00-440d-929b-a43c32e5139c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+----+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|rank|\n",
            "+-----+-----+-----------+---------------+------+------------+----+\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|   1|\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|   2|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|   3|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|   4|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|   1|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|   2|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|   1|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|   2|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|   1|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|   2|\n",
            "+-----+-----+-----------+---------------+------+------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create another view (local or global) that only contains \"Engineering\" employees."
      ],
      "metadata": {
        "id": "koJpHEgQL-mB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"employees_local1\")\n",
        "\n",
        "spark.sql(\"SELECT * FROM employees_local1 WHERE Department = 'Engineering'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV5ju9cCML2g",
        "outputId": "417e6c60-a844-4e6b-a6d8-20ef9a8ac334"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering| Data Platform| 87000|          45|\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  109| Neha|Engineering|Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a SQL view that filters out all employees working < 38 hours and saves it as \"active_employees\" ."
      ],
      "metadata": {
        "id": "-Zm_qdg1MaYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_spark.sql(\"CREATE OR REPLACE TEMP VIEW active_employees AS SELECT * FROM global_temp.employees_global WHERE HoursPerWeek >= 38;\").show()\n",
        "new_spark.sql(\"SELECT * FROM active_employees\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OB2JNsRMetO",
        "outputId": "0b0aecf3-3fff-49ea-f9b3-646f009c1af0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++\n",
            "||\n",
            "++\n",
            "++\n",
            "\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}