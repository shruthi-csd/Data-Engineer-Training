{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb33f53c-da5f-4edd-bec3-dc375e8472f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+---------+---------+\n|EmpID|  Name|Department|UnitsSold|UnitPrice|\n+-----+------+----------+---------+---------+\n| E101|  amit|     sales|       10|     1200|\n| E102| sneha| marketing|        8|     1500|\n| E103|  ravi|     sales|       12|     1300|\n| E104|anjali|        hr|        7|     1100|\n| E105|   raj|     sales|        5|     1000|\n+-----+------+----------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"E101\",\"amit\",\"sales\",10,1200),\n",
    "    (\"E102\",\"sneha\",\"marketing\",8,1500),\n",
    "    (\"E103\",\"ravi\",\"sales\",12,1300),\n",
    "    (\"E104\",\"anjali\",\"hr\",7,1100),\n",
    "    (\"E105\",\"raj\",\"sales\",5,1000)\n",
    "]\n",
    "columns = [\"EmpID\",\"Name\",\"Department\",\"UnitsSold\",\"UnitPrice\"]\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5b473e-a450-429d-a355-c36a74ed396a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#csv\n",
    "df.write.mode(\"overwrite\").option(\"header\",\"True\").csv(\"/tmp/employees_csv\")\n",
    "\n",
    "#json\n",
    "df.write.mode(\"overwrite\").json(\"/tmp/employees_json\")\n",
    "\n",
    "#parquet\n",
    "df.write.mode(\"overwrite\").parquet(\"/tmp/employees_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceb6fdeb-008e-49a6-82ad-ed3406a80c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+---------+---------+\n|EmpID|  Name|Department|UnitsSold|UnitPrice|\n+-----+------+----------+---------+---------+\n| E104|anjali|        hr|        7|     1100|\n| E105|   raj|     sales|        5|     1000|\n| E102| sneha| marketing|        8|     1500|\n| E101|  amit|     sales|       10|     1200|\n| E103|  ravi|     sales|       12|     1300|\n+-----+------+----------+---------+---------+\n\n+----------+-----+------+---------+---------+\n|Department|EmpID|  Name|UnitPrice|UnitsSold|\n+----------+-----+------+---------+---------+\n|        hr| E104|anjali|     1100|        7|\n|     sales| E105|   raj|     1000|        5|\n| marketing| E102| sneha|     1500|        8|\n|     sales| E101|  amit|     1200|       10|\n|     sales| E103|  ravi|     1300|       12|\n+----------+-----+------+---------+---------+\n\n+-----+------+----------+---------+---------+\n|EmpID|  Name|Department|UnitsSold|UnitPrice|\n+-----+------+----------+---------+---------+\n| E104|anjali|        hr|        7|     1100|\n| E105|   raj|     sales|        5|     1000|\n| E102| sneha| marketing|        8|     1500|\n| E101|  amit|     sales|       10|     1200|\n| E103|  ravi|     sales|       12|     1300|\n+-----+------+----------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# read csv\n",
    "df_csv = spark.read.option(\"header\",True).csv(\"/tmp/employees_csv\")\n",
    "df_csv.show()\n",
    "\n",
    "#read json\n",
    "df_json = spark.read.json(\"/tmp/employees_json\")\n",
    "df_json.show()\n",
    "\n",
    "#read parquet\n",
    "df_parquet = spark.read.parquet(\"/tmp/employees_parquet\")\n",
    "df_parquet.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "practice 2025-08-07 09:44:52",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}