{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup & SparkSession Initialization"
      ],
      "metadata": {
        "id": "U1wraIat4ztb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5Zl7fhA4y_y",
        "outputId": "aa01edc8-8de6-4527-f2de-f39b130659c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BotCampus PySpark Practice\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "RijwR3Ps4_8J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = [\n",
        "    (\"Anjali\", \"Bangalore\", 24),\n",
        "    (\"Ravi\", \"Hyderabad\", 28),\n",
        "    (\"Kavya\", \"Delhi\", 22),\n",
        "    (\"Meena\", \"Chennai\", 25),\n",
        "    (\"Arjun\", \"Mumbai\", 30)\n",
        "]\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"=== Schema ===\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"=== DataFrame ===\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XopSjB7a5CrK",
        "outputId": "8be1b532-6991-46b4-8180-330fe0c40033"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Schema ===\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            "\n",
            "=== DataFrame ===\n",
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Anjali|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "| Arjun|   Mumbai| 30|\n",
            "+------+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = df.rdd\n",
        "\n",
        "print(\"=== Collect Data ===\")\n",
        "print(df.collect())\n",
        "\n",
        "print(\"=== RDD Map Output (Name Only) ===\")\n",
        "print(rdd.map(lambda x: x.name).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmEB7vV95EoJ",
        "outputId": "ffa890e2-a04b-448b-f2f3-a4fa3c2a71e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Collect Data ===\n",
            "[Row(name='Anjali', city='Bangalore', age=24), Row(name='Ravi', city='Hyderabad', age=28), Row(name='Kavya', city='Delhi', age=22), Row(name='Meena', city='Chennai', age=25), Row(name='Arjun', city='Mumbai', age=30)]\n",
            "=== RDD Map Output (Name Only) ===\n",
            "['Anjali', 'Ravi', 'Kavya', 'Meena', 'Arjun']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDDs & Transformations"
      ],
      "metadata": {
        "id": "lCgawEPA5bJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = spark.sparkContext.parallelize([\n",
        "    \"Ravi from Bangalore loved the delivery\",\n",
        "    \"Meena from Hyderabad had a late order\",\n",
        "    \"Ajay from Pune liked the service\",\n",
        "    \"Anjali from Delhi faced UI issues\",\n",
        "    \"Rohit from Mumbai gave positive feedback\"\n",
        "])"
      ],
      "metadata": {
        "id": "6_dPtcdU5X-u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "words = feedback.flatMap(lambda line: line.lower().split())"
      ],
      "metadata": {
        "id": "oLnVFy9Z5jgX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = {\"from\", \"the\", \"a\", \"had\"}\n",
        "filtered_words = words.filter(lambda w: w not in stop_words)"
      ],
      "metadata": {
        "id": "VovbkiJt5kRX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = filtered_words.map(lambda w: (w, 1)).reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "print(\"=== Word Counts ===\")\n",
        "print(word_counts.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvG8rTBo5q0w",
        "outputId": "8430fbc8-7934-4e8d-dbe6-78439a796585"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Word Counts ===\n",
            "[('loved', 1), ('liked', 1), ('service', 1), ('anjali', 1), ('faced', 1), ('issues', 1), ('rohit', 1), ('mumbai', 1), ('positive', 1), ('feedback', 1), ('ravi', 1), ('bangalore', 1), ('delivery', 1), ('meena', 1), ('hyderabad', 1), ('late', 1), ('order', 1), ('ajay', 1), ('pune', 1), ('delhi', 1), ('ui', 1), ('gave', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_3 = word_counts.takeOrdered(3, key=lambda x: -x[1])\n",
        "print(\"=== Top 3 Words ===\")\n",
        "print(top_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hn7V31W5th_",
        "outputId": "b6dc9045-8380-45af-a810-4421bcd7a3c3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Top 3 Words ===\n",
            "[('loved', 1), ('liked', 1), ('service', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # DataFrames & Transformations (Joins)"
      ],
      "metadata": {
        "id": "gTzTetmd58jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "students = [\n",
        "    (\"Amit\", \"10-A\", 89),\n",
        "    (\"Kavya\", \"10-B\", 92),\n",
        "    (\"Anjali\", \"10-A\", 78),\n",
        "    (\"Rohit\", \"10-B\", 85),\n",
        "    (\"Sneha\", \"10-C\", 80)\n",
        "]\n",
        "columns = [\"name\", \"section\", \"marks\"]\n",
        "\n",
        "attendance = [\n",
        "    (\"Amit\", 24),\n",
        "    (\"Kavya\", 22),\n",
        "    (\"Anjali\", 20),\n",
        "    (\"Rohit\", 25),\n",
        "    (\"Sneha\", 19)\n",
        "]\n",
        "columns2 = [\"name\", \"days_present\"]\n",
        "\n",
        "df_students = spark.createDataFrame(students, columns)\n",
        "df_attendance = spark.createDataFrame(attendance, columns2)\n"
      ],
      "metadata": {
        "id": "-r4VyfJu5_BX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_join = df_students.join(df_attendance, \"name\")"
      ],
      "metadata": {
        "id": "AZT9ejm26PFe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_join = df_join.withColumn(\"attendance_rate\", col(\"days_present\")/25)\n",
        "df_join = df_join.withColumn(\"grade\",\n",
        "                             when(col(\"marks\")>90, \"A\")\n",
        "                             .when((col(\"marks\")>=80) & (col(\"marks\")<=90), \"B\")\n",
        "                             .otherwise(\"C\"))\n",
        "\n",
        "print(\"=== Students with Grades ===\")\n",
        "df_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5SRFhU96TDE",
        "outputId": "53cffc48-0774-48ba-c7f9-d9fd2aff6616"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Students with Grades ===\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "|  name|section|marks|days_present|attendance_rate|grade|\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "|  Amit|   10-A|   89|          24|           0.96|    B|\n",
            "|Anjali|   10-A|   78|          20|            0.8|    C|\n",
            "| Kavya|   10-B|   92|          22|           0.88|    A|\n",
            "| Rohit|   10-B|   85|          25|            1.0|    B|\n",
            "| Sneha|   10-C|   80|          19|           0.76|    B|\n",
            "+------+-------+-----+------------+---------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_poor_attendance = df_join.filter((col(\"grade\").isin(\"A\",\"B\")) & (col(\"attendance_rate\")<0.8))\n",
        "print(\"=== Good Grades but Poor Attendance ===\")\n",
        "df_poor_attendance.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugNk4Ptx6WMU",
        "outputId": "9f07440c-92a6-4de4-c6aa-1e05703aeb66"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Good Grades but Poor Attendance ===\n",
            "+-----+-------+-----+------------+---------------+-----+\n",
            "| name|section|marks|days_present|attendance_rate|grade|\n",
            "+-----+-------+-----+------------+---------------+-----+\n",
            "|Sneha|   10-C|   80|          19|           0.76|    B|\n",
            "+-----+-------+-----+------------+---------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Ingest CSV & JSON, Save to Parquet"
      ],
      "metadata": {
        "id": "6DBTm_D_6ndj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "csv_data = \"\"\"emp_id,name,dept,city,salary\n",
        "101,Anil,IT,Bangalore,80000\n",
        "102,Kiran,HR,Mumbai,65000\n",
        "103,Deepa,Finance,Chennai,72000\n",
        "\"\"\"\n",
        "with open(\"employees.csv\",\"w\") as f:\n",
        "    f.write(csv_data)"
      ],
      "metadata": {
        "id": "6m9PJ0ti6pdF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_data = {\n",
        "    \"id\": 201,\n",
        "    \"name\": \"Nandini\",\n",
        "    \"contact\": {\"email\": \"nandi@example.com\", \"city\": \"Hyderabad\"},\n",
        "    \"skills\": [\"Python\", \"Spark\", \"SQL\"]\n",
        "}\n",
        "import json\n",
        "with open(\"employee.json\",\"w\") as f:\n",
        "    json.dump(json_data, f)"
      ],
      "metadata": {
        "id": "PJnXo6RQ6wX1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
        "df_json = spark.read.json(\"employee.json\", multiLine=True)\n",
        "\n",
        "print(\"=== CSV Data ===\")\n",
        "df_csv.show()\n",
        "\n",
        "print(\"=== JSON Data ===\")\n",
        "df_json.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV0Zn44G6ycE",
        "outputId": "1b2e4a37-5d7d-4ab7-bfa8-8ba746c4fbbb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CSV Data ===\n",
            "+------+-----+-------+---------+------+\n",
            "|emp_id| name|   dept|     city|salary|\n",
            "+------+-----+-------+---------+------+\n",
            "|   101| Anil|     IT|Bangalore| 80000|\n",
            "|   102|Kiran|     HR|   Mumbai| 65000|\n",
            "|   103|Deepa|Finance|  Chennai| 72000|\n",
            "+------+-----+-------+---------+------+\n",
            "\n",
            "=== JSON Data ===\n",
            "+------------------------------+---+-------+--------------------+\n",
            "|contact                       |id |name   |skills              |\n",
            "+------------------------------+---+-------+--------------------+\n",
            "|{Hyderabad, nandi@example.com}|201|Nandini|[Python, Spark, SQL]|\n",
            "+------------------------------+---+-------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode\n",
        "\n",
        "df_flat = df_json.select(\n",
        "    col(\"id\"),\n",
        "    col(\"name\"),\n",
        "    col(\"contact.email\").alias(\"email\"),\n",
        "    col(\"contact.city\").alias(\"city\"),\n",
        "    explode(col(\"skills\")).alias(\"skill\")\n",
        ")\n",
        "\n",
        "print(\"=== Flattened JSON ===\")\n",
        "df_flat.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OuG62rp60vV",
        "outputId": "ee060b35-6a6d-4de0-b33b-7bfdcc2e364e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Flattened JSON ===\n",
            "+---+-------+-----------------+---------+------+\n",
            "| id|   name|            email|     city| skill|\n",
            "+---+-------+-----------------+---------+------+\n",
            "|201|Nandini|nandi@example.com|Hyderabad|Python|\n",
            "|201|Nandini|nandi@example.com|Hyderabad| Spark|\n",
            "|201|Nandini|nandi@example.com|Hyderabad|   SQL|\n",
            "+---+-------+-----------------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv.write.partitionBy(\"city\").mode(\"overwrite\").parquet(\"parquet_csv/\")\n",
        "df_flat.write.partitionBy(\"city\").mode(\"overwrite\").parquet(\"parquet_json/\")"
      ],
      "metadata": {
        "id": "7M_nqbap62vk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark SQL with Temp Views"
      ],
      "metadata": {
        "id": "x8cn2Qmd7EdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_students.createOrReplaceTempView(\"students_view\")"
      ],
      "metadata": {
        "id": "sl52wMKA7IRy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT section, AVG(marks) as avg_marks FROM students_view GROUP BY section\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RH3BZh87It7",
        "outputId": "d208f530-6552-4bf6-9416-04cb28ebda58"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|section|avg_marks|\n",
            "+-------+---------+\n",
            "|   10-A|     83.5|\n",
            "|   10-B|     88.5|\n",
            "|   10-C|     80.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT section, name, marks\n",
        "FROM (\n",
        "    SELECT *, RANK() OVER(PARTITION BY section ORDER BY marks DESC) as rnk\n",
        "    FROM students_view\n",
        ") tmp\n",
        "WHERE rnk=1\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlZ_O44b7I9i",
        "outputId": "6d9e42f6-35ad-4771-c911-fe4ea0c476cd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+\n",
            "|section| name|marks|\n",
            "+-------+-----+-----+\n",
            "|   10-A| Amit|   89|\n",
            "|   10-B|Kavya|   92|\n",
            "|   10-C|Sneha|   80|\n",
            "+-------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "    CASE\n",
        "        WHEN marks>90 THEN 'A'\n",
        "        WHEN marks>=80 THEN 'B'\n",
        "        ELSE 'C'\n",
        "    END as grade,\n",
        "    COUNT(*) as student_count\n",
        "FROM students_view\n",
        "GROUP BY grade\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DieO2PRv7JMS",
        "outputId": "b13dbd68-31f3-40cf-9dac-0fcd084129d2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+\n",
            "|grade|student_count|\n",
            "+-----+-------------+\n",
            "|    B|            3|\n",
            "|    A|            1|\n",
            "|    C|            1|\n",
            "+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT *\n",
        "FROM students_view\n",
        "WHERE marks > (SELECT AVG(marks) FROM students_view)\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URSNnkdW7TZL",
        "outputId": "75667984-3d8d-4e2a-de62-94a783c25435"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-----+\n",
            "| name|section|marks|\n",
            "+-----+-------+-----+\n",
            "| Amit|   10-A|   89|\n",
            "|Kavya|   10-B|   92|\n",
            "|Rohit|   10-B|   85|\n",
            "+-----+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partitioned Data & Incremental Loading"
      ],
      "metadata": {
        "id": "R83_0kNT7ibi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_students.write.partitionBy(\"section\").mode(\"overwrite\").parquet(\"output/students/\")\n"
      ],
      "metadata": {
        "id": "XdEzjdeI7lMR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "incremental = [(\"Tejas\", \"10-A\", 91)]\n",
        "df_inc = spark.createDataFrame(incremental, [\"name\", \"section\", \"marks\"])\n",
        "df_inc.write.mode(\"append\").partitionBy(\"section\").parquet(\"output/students/\")"
      ],
      "metadata": {
        "id": "d-UQFffu7sOw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Files in output/students/:\", os.listdir(\"output/students/\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPbMrHYz7uIp",
        "outputId": "780ea590-edb0-4a7c-f8bd-d9df199a9f42"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in output/students/: ['._SUCCESS.crc', 'section=10-A', 'section=10-B', '_SUCCESS', 'section=10-C']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_10A = spark.read.parquet(\"output/students/section=10-A\")\n",
        "df_10A.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tmuKh1W7v4b",
        "outputId": "4e3999ed-8c9d-4860-c27a-818c86353167"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|  name|marks|\n",
            "+------+-----+\n",
            "|Anjali|   78|\n",
            "| Tejas|   91|\n",
            "|  Amit|   89|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Count after append in 10-A:\", df_10A.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMZn0Elw7yFx",
        "outputId": "d6c7c7c0-bf9e-4f91-9428-61106347c7be"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count after append in 10-A: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL Pipeline End to End\n"
      ],
      "metadata": {
        "id": "Jw1L2gr38A2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_etl = \"\"\"emp_id,name,dept,salary,bonus\n",
        "1,Arjun,IT,75000,5000\n",
        "2,Kavya,HR,62000,\n",
        "3,Sneha,Finance,68000,4000\n",
        "4,Ramesh,Sales,58000,\n",
        "\"\"\"\n",
        "with open(\"etl_employees.csv\",\"w\") as f:\n",
        "    f.write(csv_etl)"
      ],
      "metadata": {
        "id": "OlgrvEtK8FMi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_etl = spark.read.csv(\"etl_employees.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "aBbz-NoP8LBQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_etl = df_etl.fillna({\"bonus\":2000})"
      ],
      "metadata": {
        "id": "9kzUfblV8M2I"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_etl = df_etl.withColumn(\"total_ctc\", col(\"salary\")+col(\"bonus\"))"
      ],
      "metadata": {
        "id": "NE2itA0P8O2f"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df_etl.filter(col(\"total_ctc\")>65000)\n",
        "\n",
        "print(\"=== Filtered Employees ===\")\n",
        "df_filtered.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpEb2g_X8RIA",
        "outputId": "b1e829b7-d7c2-47f3-fd02-420f15910017"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Filtered Employees ===\n",
            "+------+-----+-------+------+-----+---------+\n",
            "|emp_id| name|   dept|salary|bonus|total_ctc|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "|     1|Arjun|     IT| 75000| 5000|    80000|\n",
            "|     3|Sneha|Finance| 68000| 4000|    72000|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.write.mode(\"overwrite\").json(\"etl_json_output/\")\n"
      ],
      "metadata": {
        "id": "KBgWcWjr8T5e"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.write.partitionBy(\"dept\").mode(\"overwrite\").parquet(\"etl_parquet_output/\")\n"
      ],
      "metadata": {
        "id": "Ouj_vDV68WK3"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}